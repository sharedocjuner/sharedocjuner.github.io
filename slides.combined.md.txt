---
title: ë„˜ì¹˜ì§€ë„, ë¶€ì¡±í•˜ì§€ë„ ì•ŠëŠ” kubernetes ë‹¤ë£¨ê¸°
pdf: openinfraday-junho.pdf
slideNumber: true
controls: true
---

# ë„˜ì¹˜ì§€ë„, ë¶€ì¡±í•˜ì§€ë„ ì•ŠëŠ” kubernetes ë‹¤ë£¨ê¸° {bg=#123456  .light-on-dark}


ë‹¹ì‹ ì´ ëª¨ë¥´ë©´ ë‹¹í•˜ëŠ” 3ê°€ì§€ ì¥ì• </br>
Kubernetesì˜ Secret Recipe</br>
</span>

# Hello! I am {bg=#123456  .light-on-dark}

**Junho.son** </br>
<small> junho.son@linecorp.com</small> </br>
Work at <span style="color: green;">**Line**</span> </br>
Dev, Ops, All about the Nucleo. </br>
<img src="./assets/images/itsme.jpg" width="240" height="420">


# First met with Kubernetes. {bg=#123456 .light-on-dark}

ìˆœí’ì— ë— ë‹¨ë“¯ ì˜ ë‚˜ê°€ë˜ kubernetes </br>
<img src="./assets/images/yougetk8s.jpg" width="400" height="350"> 


# But, Some while... {bg=#123456 .light-on-dark}

<img src="./assets/images/start.jpg" width="350" height="350"> 


#  Objectitive {bg=#123456 .light-on-dark}

## 

<span style="font-family: xkcd Script; font-size: 1.5em;">
incidents teach you</br> 
how to build a <span style="color: yellow">reliable system</span></br>
</span>
</br>

ì œê°€ ê²ªì—ˆë˜ ë“±ì— ë•€ë‚˜ëŠ” ìƒí™©ì„ ì—¬ëŸ¬ë¶„ë“¤ì´ ê²ªì§€ì•Šê³ ,</br>  
ë¯¸ë¦¬ ëŒ€ì‘í•  ìˆ˜ ìˆë„ë¡ ê³µìœ í•˜ëŠ” ìë¦¬ì…ë‹ˆë‹¤.</br>  

ë§Œì•½ ì—¬ëŸ¬ë¶„ë“¤ì´ ëª¨ë‘ ê²ªì€ ì¼ì´ë¼ë©´...</br>
</span>

## 

<img src="./assets/images/thumbs-up-rambo.jpg" width="400" height="350">

ì•„ì£¼ ì¢‹ì€ í´ëŸ¬ìŠ¤í„°ë¥¼ ê°€ì§€ê³  ê³„ì‹œêµ°ìš”...ğŸ‘ 

# Index {bg=#123456 .light-on-dark}

ì˜¤ëŠ˜ì€ ì´ì•¼ê¸° í•˜ê³  ì‹¶ì€ ê²ƒë“¤: </br>

* ëª¨ë¥´ë©´ ë¬´ì¡°ê±´ ë‹¹í•˜ëŠ” kubernetes ì¥ì•   
* ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” kubernetesë¥¼ ë§Œë“¤ê¸°  
* ìš´ì˜ ìë™í™”ë¥¼ ìœ„í•œ ë‹¤ìŒ ê³„íš  


#  First, Talk about:{bg=#123456 .light-on-dark}

* ETCD mvcc: database space exceeded </br>
* Invalid master/kubelet certification </br>
* Slow scheduling and DNS failure </br>

#  ETCD database space exceeded{bg=#123456 .light-on-dark}

## 

ì˜ ë™ì‘í•˜ë˜ <span style="color:yellow;">kubernetes</span> ì—ì„œ ì•„ë˜ì˜ ì—ëŸ¬ê°€ ë°œìƒí•œë‹¤.ğŸ˜­
```
$ kc apply -f deployment/app.yaml
etcdserver: mvcc: database space exceeded

$ kc scale --current-replicas=2 --replicas=3 deployment/nucleo-flask-sample -n junho-son
etcdserver: mvcc: database space exceeded
```

## 

<span style="color:skyblue;">*ì¦ìƒ*</br></span>

<small>
<li>ETCD read only</br></li>
<li>k8s objectë“¤ì˜ update(ë³€ê²½, ìƒì„± ë“±)ì´ ë¶ˆê°€ëŠ¥í•´ì§</br></li>
</small>

<span style="color:skyblue;">*ì›ì¸*</br></span>

<small>
<li> ETCD keyspaceëŠ” key/valueì™€ í•¨ê»˜ ê·¸ê²ƒì˜ **revision history**ë¥¼ ì €ì¥í•œë‹¤.</br></li>
<li> <span style="color:yellow;"> hisotryê°€ ì¦ê°€í•¨</span> -> crd, deploymentì˜ ë¹ˆë²ˆí•œ ë³€ê²½</br></li>
<li> ì„±ëŠ¥ ì €í•˜ ë° ì €ì¥ê³µê°„ ê³ ê°ˆì„ í”¼í•˜ê¸° ìœ„í•´ ì£¼ê¸°ì ì¸ **compactionê³¼ defragment**ì´ í•„ìš”</br></li>
</small>

## 
<span style="color:skyblue;">*Solutions*</span>

* step1. etcdctlë¡œ compactionê³¼ defragmentë¥¼ í•¨
```
export PEERS="http://10.127.111.53:2379,http://10.127.114.99:2379,http://10.127.114.96:2379"
export ETCDCTL_API=3 

# compaction
$ rev=$(etcdctl  endpoint status --write-out="json" | egrep -o '"revision":[0-9]*' | egrep -o '[0-9]*')
$ etcdctl compact $rev
compacted revision 518729

# size ì•ˆ ì¤„ì—ˆìŒ
| http://10.127.111.53:2379 | 750655cd04e8a0f0 | 3.2.15 | 2.1 GB | false | 4 | 578868 |
| http://10.127.114.99:2379 | 77c94c09da32ea5a | 3.2.15 | 2.2 GB | true | 4 | 578868 |
| http://10.127.114.96:2379 | 7fa913847ab8f9f5 | 3.2.15 | 2.1 GB | false | 4 | 578868 |

# defragment(resize high watermark)
$ etcdctl --endpoints ${PEERS} defrag
Finished defragmenting etcd member[http://10.127.111.53:2379]
Finished defragmenting etcd member[http://10.127.114.99:2379]
Finished defragmenting etcd member[http://10.127.114.96:2379]

# size ì¤„ì—ˆìŒ
| http://10.127.111.53:2379 | 750655cd04e8a0f0 | 3.2.15 | 9.7 MB | false | 4 | 582117 |
| http://10.127.114.99:2379 | 77c94c09da32ea5a | 3.2.15 | 9.7 MB | true | 4 | 582117 |
| http://10.127.114.96:2379 | 7fa913847ab8f9f5 | 3.2.15 | 9.7 MB | false | 4 | 582117 |
```

## 
<span style="color:skyblue;">*Solutions*</span>

* step2. etcdctlë¡œ alarm disarmì„ í•´ì•¼ í•¨
```
etcdctl --endpoints ${PEERS} alarm list
memberID:8631513766031452762 alarm:NOSPACE
memberID:8432521691336843504 alarm:NOSPACE

etcdctl --endpoints ${PEERS} alarm disarm
memberID:8631513766031452762 alarm:NOSPACE
memberID:8432521691336843504 alarm:NOSPACE
```

## 

<span style="color:skyblue;">*Lessons Learned*</span>

kube-apiserver compaction options(default: 5m)  
```
--etcd-compaction-interval duration     Default: 5m0s
The interval of compaction requests. \
If 0, the compaction request from apiserver is disabled.
```

## 
<span style="color:skyblue;">*Lessons Learned*</span>

scheduled defrag job 

```
#!/bin/bash
# start with random sleep
sleep $((RANDOM % 10 + 1))

CMD=$(which etcdctl)
ENDPOINT=$(cat /etc/etcd/etcd.conf  | grep "ETCD_ADVERTISE_CLIENT_URLS" | cut -d"=" -f2 | tr -d "\"")
LOG="/var/log/etcd-defrag-$(date +"%Y%m%d")"

ETCDCTL_API=3 ${CMD} --endpoints http://127.0.0.1:2379 \
  defrag --command-timeout=60s 1>>${LOG} 2>&1
ETCDCTL_API=3 ${CMD} --endpoints http://127.0.0.1:2379 \
  endpoint status 1>>${LOG}

# delete log
find /var/log/etcd-defrag-* -maxdepth 1 -type f -ctime +7 -delete
```

<small>Ref: [defrag using cronjob](https://www.ibm.com/support/knowledgecenter/en/SSBS6K_3.1.0/manage_cluster/manage_etcd_clusters.html){target="_blank"}</small>

## 

<span style="color:skyblue;">*Lessons Learned*</span> 

adjust etcd db size

```
$ cat /etc/etcd/etcd.conf
...
ETCD_QUOTA_BACKEND_BYTES="4294967296"
...
```

```
--quota-backend-bytes
    Raise alarms when backend size exceeds the given quota (0 defaults to low space quota).
    default: 0
    env variable: ETCD_QUOTA_BACKEND_BYTES
```

## 

<span style="color:skyblue;">*Lessons Learned*</span> 

Monitoring

```
Each etcd server exports metrics under the /metrics path on its client port and optionally on locations given by --listen-metrics-urls.
# prometheus scrap options
scrape_configs:
  - job_name: etcd
    static_configs:
    - targets: ['10.127.157.129:2379','10.127.157.133:2379','10.127.157.137:2379']
```
```
# prometheus query
etcd_debugging_mvcc_db_total_size_in_bytes 
etcd_debugging_mvcc_keys_total
etcd_debugging_mvcc_slow_watcher_total
...
```

<small><span style="color:white">Ref: [prometheus scrap](https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/monitoring.md){target="_blank"}</span></small>

##

<span style="color:skyblue;">*Lessons Learned*</span> 

Backup

* etcd Backup
```
#!/bin/bash
BACKUP_DIR="/var/backup/etcd"
ENDPOINT=http://127.0.0.1:2379
BACKUP_FILE="${BACKUP_DIR}/$(date +'%y%m%d').db"

mkdir -p ${BACKUP_DIR}
ETCDCTL_API=3 etcdctl --endpoints=${ENDPOINT} snapshot save "${BACKUP_FILE}"

cd ${BACKUP_DIR} && etcdctl backup --data-dir /var/lib/etcd/default.etcd

sleep 1

# move member dir
mv ${BACKUP_DIR}/member ${BACKUP_DIR}/member-$(date +'%y%m%d')

# cleanup backup older than 7 days
find /var/backup/etcd -maxdepth 1 -type f -ctime +7 -delete
find /var/backup/etcd/member-* -maxdepth 0 -type d -ctime +7 -exec rm -r /"{}/" \;
```

<small>Ref: [backup and restore](https://juner417.github.io/blog/etcd-backup-and-restore/)</small>


# Invalid master/kubelet certification {bg=#123456 .light-on-dark}

##
ì–´ëŠë‚  ê°‘ìê¸° kubectlë¡œ ì¡°íšŒê°€ ë¶ˆê°€ëŠ¥ í•˜ê³ , 

nodeê°€ í•˜ë‚˜ ë‘˜ì”© NotReadyê°€ ë˜ê¸° ì‹œì‘í•œë‹¤. 

ğŸ˜­
```
$ kc get pods
Unable to connect to the server: x509: certificate is valid 

# apiserver log 
E0709 10:55:24.437376       1 authentication.go:62] Unable to authenticate the request due to an error: \
  [x509: certificate has expired or is not yet valid, x509: certificate has expired or is not yet valid]
```

## 

<span style="color:skyblue;">*ì¦ìƒ*</br></span>

<small>
<li>kube-apiserver ì ‘ê·¼ ë¶ˆê°€ëŠ¥</br></li>
<li>kubelet NotReady status</br></li>
</small>

<span style="color:skyblue;">*ì›ì¸*</br></span>
<small>
<li>master component/kubelet configì˜ ì¸ì¦ì„œ ë§Œë£Œ</br></li>
<li>kubeadmì€ master component certëŠ” 1ë…„ì§œë¦¬ë¥¼ ë§Œë“¬</br></li>
<li>kubeadm upgradeë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  upgradeë¥¼ í•¨(180ì¼ ë³´ë‹¤ ì‘ìœ¼ë©´ upgradeì‹œ ê°±ì‹ )</br></li>
</small>


## 

<span style="color:skyblue;">*Solutions*</span>

kubeadm init phase certs [master component]
```
## apiserver --cert-dirë¡œ ê¸°ì¡´ ca certë¡œ ìƒì„±í•´ì•¼ í•¨(ì•ˆí•˜ë©´ ìƒˆë¡­ê²Œ ca certìƒì„±)
kubeadm init phase certs all \
  --apiserver-advertise-address 10.20.192.31 \
  --apiserver-cert-extra-sans gonz-dev-caravan.devdev.com \
  --service-cidr 172.28.0.0/15 --cert-dir /root/test_pki/pki 
```

ìœ„ì—ì„œ ê°±ì‹ í•œ ë‚´ìš©ìœ¼ë¡œ kubelet admin.conf ë³€ê²½/ë°°í¬
```
ansible-playbook -i inventory/service-dev/host renew_cert_node.yml
```

## 

<span style="color:skyblue;">*Lessons Learned*</span> 

ë§˜í¸í•œê²Œ ì§±! 
*master component certification*ì€,

* 100ë…„ì§œë¦¬ certificationì„ ë§Œë“¤ì ğŸ˜… </br>
* kubeadmì„ ì´ìš©í•˜ì—¬ ìì£¼ upgradeë¥¼ í•˜ì(ê¶Œì¥).</br>


ê·¸ë ‡ë‹¤ë©´ <span style="color:yellow;">kubeletì€? ğŸ¤”</span>

<small>
[certificate - kubernetes the hard way](https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/master/docs/04-certificate-authority.md){target="_blank"}</br>
[upgrade using kubeadm](https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#automatic-certificate-renewal){target="_blank"}</br>
</small>

## 

<span style="color:skyblue;">*Lessons Learned*</span> 

kubelet rotate-certification ê¸°ëŠ¥ ì´ìš©</br>


* kubeletì— ```--rotate-certificates=true``` ì˜µì…˜ ì¶”ê°€
* kubelet feature-gatewayì— RotateKubeletServerCertificate=true
* RotateKubeletServerCertificate v1.12ì—ì„œ Beta
```
$ cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
[Service]
...
Environment="KUBELET_CERTIFICATE_ARGS=--rotate-certificates=true --cert-dir=/var/lib/kubelet/pki"
Environment="KUBELET_FEATURE_GATE_ARGS=--feature-gates=RotateKubeletClientCertificate=true,RotateKubeletServerCertificate=true"
```

## 

<span style="color:skyblue;">*Lessons Learned*</span> 

kubelet rotate-certification

```
## ê³¼ì •ì€ ë§ë¡œ ì„¤ëª…... í•„ìš”?
ls -l /var/lib/kubelet/pki/
total 32
-rw------- 1 root root 1183 Jun 25 18:05 kubelet-client-2018-06-25-18-05-21.pem
-rw------- 1 root root 1183 Jun 25 18:46 kubelet-client-2018-06-25-18-46-17.pem
...
lrwxrwxrwx 1 root root   59 Jun 25 18:46 kubelet-client-current.pem -> /var/lib/kubelet/pki/kubelet-client-2018-06-25-18-46-17.pem

# kubelet log
Jun 26 17:04:27 junho003-k8s-dev-jp2v-dev kubelet[12771]: I0626 17:04:27.206702   12771 transport.go:96] certificate rotation detected, shutting down client connections to start using new credentials

# kubelet config
$ cat /etc/kubernetes/kubelet.conf
...
users:
- name: default-auth
  user:
    client-certificate: /var/lib/kubelet/pki/kubelet-client-current.pem
    client-key: /var/lib/kubelet/pki/kubelet-client-current.pem
```

## 

<span style="color:skyblue;">*Lessons Learned*</span> 

ì´ëŸ°ê±° ê´€ë¦¬í•˜ê¸° ë„ˆë¬´ í˜ë“¤ë‹¤...</br>
kops(on aws), kubespray ì‚¬ìš©í•˜ì‹œë©´... </br>
í¸í• ìˆ˜ ìˆìŠµë‹ˆë‹¤. 


#  Slow scheduling and DNS failure {bg=#123456 .light-on-dark}

##

large clusterë¥¼ ë§Œë“¤ê³ (node 1000),</br>
ì„œë¹„ìŠ¤ ë°°í¬(nginx - replica 5000)ì‹œ </br>
podì˜ <span style="color:yellow">scheduling</span>ì´ ë„ˆë¬´ ëŠë¦¬ë‹¤ </br>
ì„¸ì›”ì•„ ë„¤ì›”ì•„...ğŸ˜… </br>


## 

<span style="color:skyblue;">*ì¦ìƒ*</br></span>

<small>
<li>1000ê°œ ë…¸ë“œì˜ í´ëŸ¬ìŠ¤í„°ì—ì„œ deployment ë°°í¬ì‹œ pod schedulingì´ ëŠë¦¬ë‹¤. </br></li>
</small>

<span style="color:skyblue;">*ì›ì¸*</br></span>
<small>
<li>podì´ schedulingë ë•Œ filteringê³¼ ranking(scoring)</br></li>
<li>ëª¨ë“  ë…¸ë“œ(1000)ë¥¼ <span style="color:yellow;">filtering</span>í•˜ê³  <span style="color:yellow;">ranking</span>ì„ ê³„ì‚°í•˜ë‹¤ ë³´ë‹ˆ ì‹œê°„ì´ ì˜¤ë˜ê±¸ë¦¼</br></li>
</small>

<small>
Ref</br>
  [scheduler algorithm](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduler_algorithm.md){target="_black"}</br>
  [scheduler details](https://medium.com/@dominik.tornow/the-kubernetes-scheduler-cd429abac02f){target="_black"}</br>
</small>

##  

<span style="color:skyblue;">*Solutions*</span>

kube-schedulerì—ì„œ </br>
<span style="color:yellow;">percentageOfNodeToScore</span> ì˜µì…˜ ì¡°ì •

```
# cat schedulerconfig.yaml
...
percentageOfNodesToScore: 10
...
```
<aside class="notes">
ì´ ì˜µì…˜ì€ schedulerê°€ podì„ schedulingí• ë•Œ, ì „ì²´ë…¸ë“œê°€ ì•„ë‹Œ ì§€ì •í•œ ë…¸ë“œë§Œí¼ í™•ì¸ ë˜ë©´ ë…¸ë“œì˜ scoreë¥¼ ë§¤ê¸´ë‹¤.
</aside>

##

<img src="./assets/images/park.jpg" width="350" height="350"> 


ë­ìš”? 

## 

<span style="color:skyblue;">*Lessons Learned*</span> 

k8s pod scheduling

<img src="./assets/images/kube-scheduling.png" width="700" height="450">

<small>
Ref: [k8s custom scheduler](https://banzaicloud.com/blog/k8s-custom-scheduler/){target="_blank"}</br>
Ref: [kubernetes scheduling](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduler.md){target="_blank"}</br>
</small>

<aside class="notes">
1. podì˜ ìƒì„± ìš”ì²­ì´ ë“¤ì–´ì˜¤ë©´, podì˜ ì •ë³´ë¥¼ etcdì— ì €ì¥í•œë‹¤ ì´ë•Œ node name ì—†ë‹¤. </br>
2. scheduler podë“¤ì„ watchí•˜ëŠ”ë°, nodeê°€ boundë˜ì§€ ì•ŠëŠ” podì„ í™•ì¸í•œë‹¤. </br>
3. schedulerì—ì„œ ì í•©í•œ ë…¸ë“œë“¤ì„ filteringí•˜ê³ , scoreë¥¼ ë§¤ê²¨ podì— ì í•©í•œ nodeë“¤ì´ ì ìˆ˜ë¥¼ ê³„ì‚°í•œë‹¤. </br>
4. 3ì—ì„œ ì°¾ì€ nodeë“¤ì„ apiserverì—ê²Œ ì•Œë ¤ì£¼ì–´ binding(etcdì— ì €ì¥)í•˜ê²Œ í•œë‹¤. </br>
5. kubeletì€ apiserverë¥¼ í†µí•´ bound pod ì •ë³´ë¥¼ watchingí•˜ê³ , containerë¥¼ ì‹¤í–‰í•œë‹¤. </br>
</aside>


##  

<span style="color:skyblue;">*Lessons Learned*</span> 

Scheduler Performance Tuning

* <span style="font-family: sans-serif;"> percentageOfNodesToScore - k8s v1.12ì— ì¶”ê°€ëœ ê¸°ëŠ¥ </span>
* <span style="font-family: sans-serif;"> ì´ì „ì—ëŠ” ë¬´ì¡°ê±´ ëª¨ë“  ë…¸ë“œë¥¼ filtering, rankingí•¨</span>
  * <span style="font-family: sans-serif;"> ëª¨ë“  ë…¸ë“œì˜ feasibiltyë¥¼ í™•ì¸í•˜ëŠ” - <span style="color:yellow">filtering</span> </span>
  * <span style="font-family: sans-serif;"> feasibilityê°€ ì¶©ì¡±í•œ ë…¸ë“œë“¤ì— ëŒ€í•´ì„œ scoreë¥¼ ë§¤ê¸°ëŠ” - <span style="color:yellow"> ranking</span> </span>

## 

<span style="color:skyblue;">*Lessons Learned*</span> 

Scheduler Performance Tuning

* percentageOfNodesToScoreë¥¼ ì ìš©í•˜ë©´, filteringì‹œ ëª¨ë“  ë…¸ë“œì˜ feasibilty ëŒ€ìƒìœ¼ë¡œ í•˜ì§€ ì•Šê³ ,</br>
* percentageOfNodesToScore(%)ê°¯ìˆ˜ë§Œí¼ filteringì´ ë˜ë©´, í•´ë‹¹ ë…¸ë“œë“¤ì„ ì ìˆ˜ë¥¼ ë§¤ê²¨ rankingí•œë‹¤</br>

<small>
Ref: [scheduler perf tuning](https://kubernetes.io/docs/concepts/configuration/scheduler-perf-tuning/){target="_blank"}
</small>

##

ê·¸ëŸ°ë° v1.14 ë¶€í„° </br>
ì´ í´ëŸ¬ìŠ¤í„° ì‚¬ì´ì¦ˆë¥¼ ì´ìš©í•˜ì—¬ ì´ ê°’ì„ ê³„ì‚°í•˜ëŠ” ê³µì‹ì´ ì¶”ê°€ ëë‹¤ í•˜ë„¤ìš”...</br>

## 

ë‹¹ì‹ ì˜ DNSëŠ” ì•ˆë…•í•˜ì‹­ë‹ˆê¹Œ?</br>

ë³´í†µ ```dnsPolicy: clusterFirst``` ì„¤ì •</br>

<small>
ë§Œì•½ dbë¥¼ ì‚¬ìš©í•˜ëŠ” java spring service ë“¤ì´ ë™ì‹œì— 100ê°œ ì´ìƒ ë°°í¬ëœë‹¤ë©´?</br>
 -> dns podë“¤ì´ ì£½ì–´ë‚˜ê°€ë©´ì„œ, service podë“¤ì´ CrashloopbackOff...</br>
 -> ë‚œ ë´¤ì–´... ê·¸ ì”ì¸í•œ ëª¨ìŠµì„... ì„œë¡œê°€ ì„œë¡œë¥¼ ì£½ì´ëŠ”... ê·¸ ëª¨ìŠµì€ ë§ˆì¹˜</br>
</small>

##

![](./assets/images/dogground.jpg)

## 

<span style="color:skyblue;">*ì¦ìƒ*</br></span>

<small>
<li> dnsë„ ì£½ê³ , service podë„ ì£½ê³  ìš°ë¦¬ ëª¨ë‘ ì£½ì...</br></li>
</small>

<span style="color:skyblue;">*ì›ì¸*</br></span>
<small>
<li> java processì—ì„œ ë™ì‹œì— kube-dnsì— ì¿¼ë¦¬í•¨</br></li>
<li> 5 dot ë¯¸ë§Œ(default)ì€ cache(dnsmasq) í•œë²ˆ ì¡°íšŒí•˜ê³  upstream host /etc/resolve.conf ì¡°íšŒ</br></li>
<li> 150ê°œì˜ java pod(sidecar 3)ì´ ë™ì‹œì— 5ê°œ pod kube-dns ì¿¼ë¦¬ </br></li>
</small>

## 

<span style="color:skyblue;">*Solutions*</span>

ê±±ì •í•˜ì§€ ë§ˆì‹œê³  <span style="color:yellow;">cluster-proportional-autoscaler</span> í•œëŒ€ ë“¤ì´ì„¸ìš”... </br>

ì•„ë‹ˆë©´, Pod ```.spec.dnsConfig```ë¥¼ ì´ìš©í•˜ì—¬ ndots ì˜µì…˜ì„ ìˆ˜ì •</br>


##

```
...
    spec:
      containers:
      - command:
        - /cluster-proportional-autoscaler
        - --namespace=kube-system
        - --configmap=coredns-autoscaler     
        - --target=deployment/coredns         # ìì‹ ì˜ cluster dns target
        - --default-params={"linear":{"coresPerReplica":256,"nodesPerReplica":16,"preventSinglePointFailure":true}}
        - --logtostderr=true
        - --v=2
        image: k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.2.0
...
```
<small>
Ref: [cluster proportional autoscaler](https://github.com/kubernetes-incubator/cluster-proportional-autoscaler){target="_blank"}
</small>

## 

default paramì˜ <span style="color:yellow;">coresPerReplica</span>, <span style="color:yellow;">nodesPerReplica</span> ê°’ìœ¼ë¡œ ì ì ˆí•œ podì˜ ê°œìˆ˜ë¥¼ ê³„ì‚°í•œ í›„</br>
dns deploy ê°’ì„ ì¡°ì •í•©ë‹ˆë‹¤.</br>

```
replicas = max( ceil( cores * 1/coresPerReplica ) ,
                ceil( nodes * 1/nodesPerReplica ) )
```

<small>
Ref: [cluster-proportional-autoscaler](https://kubernetes.io/docs/tasks/administer-cluster/dns-horizontal-autoscaling/)
</small>


##

ndots 3ê°œ ë¯¸ë§Œì´ë©´, upstream dnsë¥¼ ë³´ê²Œ ì¡°ì •(absoulute name)
```
apiVersion: v1
kind: Pod
metadata:
  namespace: default
  name: example
spec:
  containers:
    - name: test
      image: nginx
  dnsConfig:
    options:
      - name: ndots
        value: "3"
```

<small>
Ref: [ndots option affect your application performances](https://pracucci.com/kubernetes-dns-resolution-ndots-options-and-why-it-may-affect-application-performances.html){target="_blank"}
</small>

#  ì‹ ë¢°í• ìˆ˜ ìˆëŠ” kubernetesë¥¼ ë§Œë“¤ê¸°</span>{bg=#123456 .light-on-dark}


#  Nginx ingress tunes </span>{bg=#123456 .light-on-dark}

## 

<span style="color:skyblue;">*í•„ìš”ì„±*</span>

* ì‚¬ìš©ìê°€ local ë¨¸ì‹ (ë™ì¼ IP)ì—ì„œ ë¶€í•˜í…ŒìŠ¤íŠ¸ ì§„í–‰, íŠ¹ì •í•œ nginxë¡œ ë¦¬í€˜ìŠ¤íŠ¸ê°€ ëª°ë¦¬ê²Œ ëœë‹¤. 
* ì´ë•Œ íŠ¹ì •í•œ nginxëŠ” ì´ëŸ° trafficì„ backendë¡œ ë³´ë‚´ë‚˜ backendê°€ ì´ë¥¼ ì²˜ë¦¬í•˜ì§€ ëª»í•˜ê²Œ ë˜ë©´, ìˆœì‹ê°„ì— <span style="color:yellow">DDos</span> ê³µê²©ì²˜ëŸ¼ ë˜ì–´ ë²„ë¦¼
* í•˜ë‚˜ì˜ ipì—ì„œ ë™ì‹œì— ë“¤ì–´ì˜¤ëŠ” ì»¤ë„¥ì…˜ ê°¯ìˆ˜ì˜ ì¡°ì •ì´ í•„ìš”.  

<aside class="notes">
* nginx-ingress controllerì˜ 503, 504 ë°œìƒ
* í•´ë‹¹ threadì˜ processê°€ reloadë„ ì•ˆë¨. 
</aside>

## 

<span style="color:skyblue;">*ì¦ìƒ*</span>

<small>
ì‚¬ìš©ìê°€ 504 gateway timeout ì—ëŸ¬ í™•ì¸</br>
kibanaë¥¼ í™•ì¸í•´ ë³´ë‹ˆ 15:38~15:41ì‚¬ì´ì— ingress í•œ ì¥ë¹„ì—ì„œ 503(4000ê±´) 504(15ê±´) ë°œìƒ</br>
í•´ë‹¹ ì¥ë¹„ì— nginx í”„ë¡œì„¸ìŠ¤ í™•ì¸í•´ë³´ë‹ˆ ë‹¤ë¥¸í”„ë¡œì„¸ìŠ¤ì™€ ìƒì„±ì‹œê°„ì´ ë§ì´ ì°¨ì´ë‚˜ëŠ” í”„ë¡œì„¸ìŠ¤ í™•ì¸</br>
(reload í•˜ì§€ ëª»í•œ í”„ë¡œì„¸ìŠ¤ê°€ ìƒê¹€)
</small>
```
$ ps -ef | grep nginx
65534     10022 131013  0 15:37 ?        00:00:57 nginx: worker process <-
65534     11562 131013  1 15:38 ?        00:01:43 nginx: worker process <-
65534     62933 131013  0 17:41 ?        00:00:00 nginx: worker process
65534     62934 131013  0 17:41 ?        00:00:00 nginx: worker process
65534     62935 131013  0 17:41 ?        00:00:00 nginx: worker process

# í•´ë‹¹ í”„ë¡œì„¸ìŠ¤  SIGTERM(kill -15) ë¡œ ì£½ì¸ í›„ ìœ„ ì—ëŸ¬ ë°œìƒì•ˆí•¨
```
<small>
ë¡œê·¸ í™•ì¸í•´ ë³´ë‹ˆ ë¶€í•˜í…ŒìŠ¤íŠ¸ê°€ ìˆì—ˆê³ , backend podì€ hpaë¡œ í™•ì¥ ëìœ¼ë‚˜,</br>
nginxëŠ” ê·¸ ì´í›„ ì´ìƒ í”„ë¡œì„¸ìŠ¤ê°€ ë˜ê³  ì •ìƒ ë™ì‘ í•˜ì§€ ëª»í•¨
</small>

## 

ë™ì¼ ipë¡œ ë†’ì€ ë¶€í•˜ê°€ ì˜¤ëŠ” ìƒí™©(DDos)ì„ ë§‰ê¸° ìœ„í•œ nginx íŠœë‹
metadata.annotationì— ì•„ë˜ì˜ ê°’ ì¡°ì •

```
nginx.ingress.kubernetes.io/limit-connections(ê°œìˆ˜) : 
  # í•˜ë‚˜ì˜ ipì—ì„œ í—ˆìš©ë˜ëŠ” ë™ì‹œ ì—°ê²° ê°œìˆ˜ -> limit_conn
nginx.ingress.kubernetes.io/limit-rps(ê°œìˆ˜) : 
  # ë§¤ì´ˆë‹¹ ì£¼ì–´ì§„ ipì—ì„œ ë¶€í„° í—ˆìš©ë˜ëŠ” ì—°ê²°ì˜ ìˆ˜   -> limit_req 
nginx.ingress.kubernetes.io/limit-rpm(ê°œìˆ˜) : 
  # ë§¤ë¶„ë‹¹ ì£¼ì–´ì§„ ipì—ì„œ ë¶€í„° í—ˆìš©ë˜ëŠ” ì—°ê²°ì˜ ìˆ˜  -> limit_req
```
* [nginx ingress rate limiting](https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md#rate-limiting)

## 

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
  ...
    'nginx.ingress.kubernetes.io/limit-connections' : '300',
    'nginx.ingress.kubernetes.io/limit-rpm' : '3500',
    'nginx.ingress.kubernetes.io/limit-rps' : '500',
  ...
```


# Pod QoS {bg=#123456 .light-on-dark}

##

<span style="color:skyblue;">*í•„ìš”ì„±*</span>

* k8sì— ì˜¬ë¼ê°„ appë“¤ì˜ reource ê²½í•©ì‹œ ì¤‘ìš”í•œ appì˜ oom ë°©ì§€

## 

<span style="color:skyblue;">*QoS ë¶„ë¥˜ ë° ì„¤ì •*</span>

* Podì€ 3ê°œì˜ QoS classë¥¼ ê°–ëŠ”ë‹¤.
  * <span style="color:yellow;">Guaranteed</span> - limit/requestë¥¼ ëª¨ë‘ ë™ì¼í•œ ê°’ìœ¼ë¡œ ì¤€ ê²½ìš°,
  * <span style="color:yellow;">Burstable</span> - requestëŠ” ì§€ì •ë˜ì–´ ìˆìœ¼ë‚˜ limitê°€ ì—†ëŠ”ê²½ìš°, í˜¹ì€ ê°™ì§€ê°€ ì•Šì€ ê²½ìš°, limitë¥¼ ì§€ì •ì•ˆí•˜ë©´ node capacityê°€ ì§€ì •ë¨
  * <span style="color:yellow;">BestEffort</span> - request/limitê°€ ëª¨ë‘ ì§€ì • ì•ˆëœê²½ìš°




## 

<span style="color:skyblue;">*Resource*</span>

* <span style="color:yellow;">compressible</span>: cpuëŠ” ë¦¬ì†ŒìŠ¤ ë³´ì¥ì´ ì¶©ì¡±ë˜ì§€ ì•Šì„ ê²½ìš°, podì´ ì¢…ë£Œë˜ì§€ ì•ŠëŠ”ë‹¤. ì¼ì‹œì ìœ¼ë¡œ throttring ë  ë¿ì´ë‹¤.
* <span style="color:yellow;">incompressible</span>: memoryì€ ì••ì¶•í• ìˆ˜ ì—†ëŠ” ë¦¬ì†ŒìŠ¤ì´ê¸° ë•Œë¬¸ì— ë¶„ë¥˜ê°€ ìˆë‹¤.

## 

<span style="color:skyblue;">*ë™ì‘ë°©ì‹*</span>

kubernetesëŠ” QoS ë³„ë¡œ OOM_SCORE_ADJë¥¼ ë‹¤ë¥´ê²Œ ì„¤ì •í•˜ì—¬ 
linux oom kill ìš°ì„ ìˆœìœ„ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤.

```
# ê°’ì´ 1000ì— ê°€ê¹Œìš¸ìˆ˜ë¡ oom kill ëŒ€ìƒ
-1000 <= OOM_SCORE_ADJ <= 1000
```

<small>
[how is kernel oom score calculated](https://serverfault.com/questions/571319/how-is-kernel-oom-score-calculated)
</small>

## 

<span style="color:skyblue;">BestEffort</span>
```
OOM_SCORE_ADJ: 1000
```

  * ë…¸ë“œì˜  ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•œ ê²½ìš° ì œì¼ ë¨¼ì € ì£½ìŒ lowest priority, ì´ ì»¨í…Œì´ë„ˆëŠ” ë…¸ë“œì˜ ì—¬ìœ  ë©”ëª¨ë¦¬ ë§Œ ì‚¬ìš©í• ìˆ˜ ìˆìŒ

##

<span style="color:skyblue;">Burstable</span>

```
2<OOM_SCORE_ADJ<999, 

min(max(2, 1000 - (1000 * memoryRequestBytes) / machineMemoryCapacityBytes), 999))
```

  * ìµœì†Œí•œì˜ ë¦¬ì†ŒìŠ¤ë¥¼ ë³´ì¥í•œë‹¤. í•˜ì§€ë§Œ ê°€ëŠ¥í•˜ë‹¤ë©´ ë” ë§ì€ ë¦¬ì†ŒìŠ¤ë¥¼ ì‚¬ìš©í• ìˆ˜ ìˆìŠµë‹ˆë‹¤(limit - node capacity). 
  * ì‹œìŠ¤í…œë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•  ê²½ìš°, ê·¸ë¦¬ê³  BestEffortê°€ ì—†ì„ ê²½ìš°, requestë¥¼ ë„˜ì–´ì„  ì‚¬ìš©ëŸ‰ì´ë©´ ì´ ì»¨í…Œì´ë„ˆê°€ ì£½ì„ ê²ƒì´ë‹¤.
  * í˜„ì œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì— ë”°ë¼ ê³„ì‚°ë˜ëŠ” ë°©ì‹ì´ ë‹¤ë¦„.

## 

<span style="color:skyblue;">Guaranteed</span>

```
OOM_SCORE_ADJ:-998
```

  * ìµœìš°ì„ ìˆœìœ„(top priority)ë¡œ ê°„ì£¼ë˜ë©° limitë¥¼ ì´ˆê³¼í•˜ì§€ ì „ê¹Œì§€ ì£½ì§€ ì•ŠëŠ”ë‹¤.</br> 

**kubelet, dockerëŠ”**
OOM_SCORE_ADJ; -999

## 

OOM_SCORE_ADJ ê°’ì„ í™•ì¸í•˜ê³  ì‹¶ì„ë•ŒëŠ”...
```
docker inspect CONTAINERID | grep -i oom
cat /proc/PID/oom_score_adj
```

* [resource QoS](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/resource-qos.md#qos-classes)
* [OOM_Killer](https://linux-mm.org/OOM_Killer)
* [OOM killer and overcommit](http://blog.lastmind.io/archives/188)


# ETCD {bg=#123456 .light-on-dark}

# ETCD Tuning {bg=#123456 .light-on-dark}

## Diskë‚˜ Networkìƒíƒœì— ë”°ë¥¸ Tune

## 

<span style="color:skyblue;">*í•„ìš”ì„±*</span>

SSD ì„œë²„ê°€ ì—†ë‹¤ë©´...?</br>
ETCD ì´ì¤‘í™”ë¥¼ ìœ„í•´ ë…¸ë“œë¥¼ ë‹¤ë¥¸ zoneì— ë‘ê³  ì‹¶ë‹¤ë©´...</br>
ì•„ë˜ì™€ ê°™ì€ warnì´ etcdì—ì„œ ìì£¼ ë°œìƒí•œë‹¤ë©´... </br>
```
2017-04-12 03:04:09.678778 W | etcdserver: \
failed to send out heartbeat on time \
(deadline exceeded for 185.938874ms)
```

disk(SSDê¶Œì¥) <span style="color:yellow;">file I/O</span>ì™€ <span style="color:yellow;">network latency</span>ë¥¼ ê³ ë¯¼ </br>
*ë¬¼ë¦¬ì ì¸ êµì²´ê°€ ë‹¹ì¥ í˜ë“¤ë‹¤ë©´ etcd tuning*

## 

Let's get some tune</br>

<span style="color:yellow;">heartbeat interval</span> : leaderê°€ followì—ê²Œ ìì‹ ì˜ ë¦¬ë”ì„ì„ ì•Œë¦¬ëŠ” ì£¼ê¸°(default: 100ms)</br>
<span style="color:yellow;">election timeout</span> : followerê°€ leader ì„ ì¶œì„ í•˜ê¸° ì „ê¹Œì§€ ê¸°ë‹¤ë¦¬ëŠ” ì‹œê°„(default: 1000ms)</br>

* leaderëŠ” heartbeatì— metadataë¥¼ í•¨ê»˜ ì „ì†¡í•¨. 


## 

<span style="color:yellow;">heartbeat interval</span>

ë„ˆë¬´ ì‘ìœ¼ë©´ ìì£¼ ë³´ë‚´ì„œ cpu/network resource ë§ì´ ì‚¬ìš©</br>
ë„ˆë¬´ í¬ë©´ leader failì„ ëŠ¦ê²Œ ê°ì§€</br>

<small>
Guide: memberë“¤ ê°„ í‰ê·  round-trip time(using ping) í‰ê· ì˜ max
</small>

<span style="color:yellow;">election timeout</span>

<small>
Guide: rrtì˜ ìµœì†Œ 10ë°° ì´ìƒ
</small>

## 

```
$ cat /etc/etcd/etcd.conf
[member]
...
ETCD_HEARTBEAT_INTERVAL="500"
ETCD_ELECTION_TIMEOUT="3000"
```

## 

ë§Œì•½ diskì˜ metricì„ ë³´ë ¤ë©´...
```
## disk 
etcd_disk_wal_fsync_duration_seconds < 10ms
etcd_disk_backend_commit_duration_seconds < 25ms
```


#  Service Lifecycle ê´€ë¦¬ {bg=#123456 .light-on-dark}

# Initial delay {bg=#123456 .light-on-dark}

## 

<span style="color:skyblue;">*í•„ìš”ì„±*</span>

server processì—ê²Œ ì‹œì‘í•  ì‹œê°„ì„ ì£¼ì </br>

## 

java spring processì˜ ê²½ìš° server portê°€ LISTEN ì´ì–´ë„</br>
server initializing í•˜ëŠ” ì‹œê°„ì´(DB ì ‘ì† ë“±) í•„ìš”í•˜ë‹¤.</br>

##

```
$ cat nucleo/app.yml
...
# Initial Delay Second(default: 30s)
# If you need to more than 30 sec to initialize your server process.
initial_delay: 60

# Health check path(default: /)
# if you don't want to send health checking request on root path(/).
# Important! App have to exist a path you specify.
health_path: /healthz
```

##

```
$ kc get pod \
 nucleo-flask-sample-371dac7-default-68f5674455-jnh6n -n junho-son -o yaml
...
    livenessProbe:
      failureThreshold: 3
      httpGet:
        path: /healthz
        port: 8080
        scheme: HTTP
      periodSeconds: 10
      initialDelaySeconds: 60
...
```

#  History ê´€ë¦¬ {bg=#123456 .light-on-dark}

## 

<span style="color:skyblue;">*í•„ìš”ì„±*</span>

* ì´ì „ ë²„ì „ replicaset, ì‹¤íŒ¨í•œ job pod, helm releaseë“± <span style="color:yellow;">history</span>ì„± ëª©ë¡ë“¤ì´ ë‚¨ì•„ìˆìŒ
* ì•ˆì§€ìš°ë©´ etcd, api list ì‹œ ë¬¸ì œê°€ ìƒê¸´ë‹¤. 


## 

* deployment</br>
```
spec.revisionHistory
```
* batchjob</br>
```
spec.[activeDeadlineSeconds | backoffLimit]
```
* cronjob</br> 
```
spec.[successfulJobsHistoryLimit(3) | failedJobsHistoryLimit(1)]
```
* helm</br> 
```
helm init --history-max 10
```

##

<span style="color:skyblue;">*ì ìš©*</span>

```
## deploy revisionHistory
$ kc get deployment -n junho-son meister-f778746-default -o yaml
apiVersion: extensions/v1beta1
kind: Deployment
...
spec:
  progressDeadlineSeconds: 600
  replicas: 3
  revisionHistoryLimit: 3
...

## batch job
$ kc get job -n junho-son block-junho-son-mysql-dev-my-mysqldump-1547071200 -o yaml
apiVersion: batch/v1
kind: Job
...
spec:
  backoffLimit: 1

## install tiller
$ helm init --history-max 10
```


# The Future of Operating {bg=#123456 .light-on-dark}
* operator sdk
* kustomize + argo
* cluster auto scaling
* admin tools(app diagnostics)


# Q & A {bg=#123456 .light-on-dark}


#  Pod Priority and PriorityClass {bg=#123456 .light-on-dark}

## 

<span style="color:skyblue;">*í•„ìš”ì„±*</span>

* pod schedulingì‹œ ìƒëŒ€ì  ìš°ì„ ìˆœìœ„(weight)ë¥¼ ì§€ì •
* ì¤‘ìš”í•œ ì„œë¹„ìŠ¤ì— ìš°ì„ ìˆœìœ„ë¥¼ ë§¤ê²¨, ë¨¼ì € ìŠ¤ì¼€ì¤„ë§ ë˜ê²Œ í•œë‹¤.

* í´ëŸ¬ìŠ¤í„°ì— allocable resourceê°€ ë¶€ì¡±í•  ê²½ìš°, 
* ìš°ì„ ìˆœìœ„ê°€ ë†’ì€ podì´ pendingì´ë¼ë©´, ë‚®ì€ìˆœìœ„ podì´ preemptionë¨
* priorityclassë³„ Quota ì§€ì •ê°€ëŠ¥(v1.13 beta)

## 

<span style="color:skyblue;">*ë™ì‘ë°©ì‹*</span>

* podì„ ë…¸ë“œì— ìŠ¤ì¼€ì¤„ë§í• ë•Œ ì‚¬ìš©í•˜ëŠ” ìš°ì„ ìˆœìœ„(weight)
* PriorityClass(1.14 stable)ë¡œ pod .specì— ì§€ì •
* ìŠ¤ì¼€ì¤„ë§ ìˆœì„œ ì„ ì ê³¼ ë¦¬ì†ŒìŠ¤ ë¶€ì¡±ì‹œ preemption/evition ìˆœì„œì— ì˜í–¥ì„ ì¤€ë‹¤. 
* High priorityê°€ pendingë˜ê³  ìˆìœ¼ë©´, Low priorityëŠ” preemptionë  í™•ë¥ ì´ ë†’ë‹¤. 
* QoS objectì™€ ë…ë¦½ì ì´ê³ , namespaceì™€ ë³„ê°œ

## 


```
$ kc get pc
NAME                      VALUE        GLOBAL-DEFAULT   AGE
system-cluster-critical   2000000000   false            135d
system-node-critical      2000001000   false            135d

$ kc get pod --all-namespaces -o \
  jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.spec.priorityClassName}{"\n"}{end}'
kube-apiserver-junho-devel001	system-cluster-critical
kube-apiserver-junho-devel002	system-cluster-critical
kube-apiserver-junho-devel003	system-cluster-critical
```

## 

<span style="color:skyblue;">ë™ì‘ ë°©ì‹ - ìŠ¤ì¼€ì¤„ë§</span>

* schedulerëŠ” pending podë“¤ì˜ priorityë¥¼ ê°€ì§€ê³  ìˆœì„œë¥¼ ì •í•œë‹¤. 
* ë†’ì€ ìš°ì„ ìˆœìœ„ì˜ podì´ ìŠ¤ì¼€ì¤„ë§ ìš”êµ¬ì‚¬í•­ì„ ë§Œì¡±í•˜ë©´ ë¨¼ì € ìŠ¤ì¼€ì¤„ë§ë¨
* ë§Œì•½ ìš”êµ¬ì‚¬í•­ì„ ë§Œì¡± ëª»í•  ê²½ìš° preemptionì´ ë™ì‘í•˜ì—¬ ë‚®ì€ ìš°ì„ ìˆœìœ„ë¥¼ preemptioní•œë‹¤. 
* preemptionì„ í•´ë„ ë§Œì¡± ëª»í•˜ë©´ *ë‹¤ìŒ ìš°ì„ ìˆœìœ„ pod*ì´ ìŠ¤ì¼€ì¤„ë§ëœë‹¤. 

## 

<span style="color:skyblue;">íŠ¹ì§•</span>

* namespaceì— ì œì•½ ë°›ì§€ ì•ŠëŠ” object.
* name(.metadata.name)ìœ¼ë¡œë¶€í„° ìš°ì„ ìˆœìœ„ ì •ìˆ˜ ê°’(.value)ì„ priorityì— ë§¤í•‘í•˜ì—¬ ì •ì˜í•œë‹¤.(by admission controller)
* pod specì— ì¡´ì¬ í•˜ì§€ ì•ŠëŠ” priorityclassê°€ ì§€ì •ë  ê²½ìš° reject(by admission controller)
* system priority class(system-node-critical, cluster)ì€ kube-systemì—ì„œë§Œ ì‚¬ìš© ê°€ëŠ¥í•˜ë‹¤. [ì°¸ê³ ](https://github.com/kubernetes/kubernetes/issues/78383)



## 
```
type PodSpec struct {
  ...
  PriorityClassName string
  Priority          *int32  // Populated by Admission Controller. Users are not allowed to set it directly.
}
```

## 
```
type PriorityClass struct {
  metav1.TypeMeta
  // +optional
  metav1.ObjectMeta
  
  // The value of this priority class. This is the actual priority that pods
  // receive when they have the above name in their pod spec.
  Value        int32
  GlobalDefault     bool
  Description       string
}
```

## 

<span style="color:skyblue;">í…ŒìŠ¤íŠ¸ </span>

ëª©ì </br>

* ì‚¬ìš©ì ì•±ì€ github repo ì´ë¦„ìœ¼ë¡œ namespaceê°€ ë‚˜ë‰˜ëŠ”ë°,
* ê·¸ namespaceì— QoSë³„ ë‹¤ë¥¸ <span style="color:yellow;">priority quota</span>ë¥¼ ê°–ê²Œí•˜ì. 

##

priority class ìƒì„±</br>
```
$ kc get priorityclass
NAME                      VALUE        GLOBAL-DEFAULT   AGE
bustable                  1000         false            2s
guaranteed                900000000    false            2s
system-cluster-critical   2000000000   false            23d
system-node-critical      2000001000   false            23d
```

## 

Quota ìƒì„± ë° priorityclass ì§€ì •</br>
```
$ cat namespace-quota.yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: test-quota1
---
apiVersion: v1
kind: List
items:
- apiVersion: v1
  kind: ResourceQuota
  metadata:
    name: pods-guaranteed
    namespace: test-quota1
  spec:
    ...
    scopeSelector:
      matchExpressions:
      - operator : In
        scopeName: PriorityClass
        values: ["guaranteed"]
- apiVersion: v1
  kind: ResourceQuota
  metadata:
    name: pods-bustable
    namespace: test-quota1
  spec:
    ...
    scopeSelector:
      matchExpressions:
      - operator : In
        scopeName: PriorityClass
        values: ["bustable"]
```

##

deployment ë°°í¬ 

```
# deployment for guaranteed
$ cat test-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment-on-guaranteed
  namespace: test-quota1
  labels:
    app: nginx
spec:
  replicas: 3
  ...
    spec:
      ...
      priorityClassName: guaranteed # priority ì§€ì •

# deployment for bustable
$ cat test-deployment-bustable.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment-on-bustable
  namespace: test-quota1
  labels:
    app: nginx2
spec:
  replicas: 3
    ...
    spec:
      ...
      priorityClassName: bustable  # priority ì§€ì •
```

##

ë°°í¬

```
$ kc create -f namespace-quota.yaml
$ kc create -f test-deployment-guaranteed.yaml
$ kc create -f test-deployment-bustable.yaml
$ kc describe quota -n test-quota1
Name:       pods-bustable
Namespace:  test-quota1
Resource    Used  Hard
--------    ----  ----
cpu         3     10
memory      3Gi   10Gi
#-> bustable pod 3ê°œ ì¡í˜

Name:       pods-guaranteed
Namespace:  test-quota1
Resource    Used    Hard
--------    ----    ----
cpu         1500m   10
memory      1500Mi  10Gi
pods        3       20
#-> guaranteed pod 3ê°œ ì¡í˜
```

##

* Ref: 
 - [í…ŒìŠ¤íŠ¸ ìƒì„¸](https://juner417.github.io/blog/resourcequota-per-priority/){target="_blank"}
 - [priority class design](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/pod-priority-api.md#priority-classes){target="_blank"}

# Prevent Node Failure </br> and </br> Managing Resource</br>{bg=#123456 .light-on-dark}

#  Kubelet GC {bg=#123456 .light-on-dark}


## 

ì§€ê¸‹ì§€ê¸‹í•œ node diskê´€ë¦¬... ì´ì   GCë¡œ í•´ê²°</br>


<span style="color:skyblue;">*í•„ìš”ì„±*</span>

* Nodeì— resource ë¶€ì¡± ëŒ€ë¹„


<small>
Ref</br>
[image collection](https://github.com/kubernetes/kubernetes/blob/v1.5.3/pkg/kubelet/images/image_gc_manager.go#L57){target="_blank"}</br>
[container collection](https://github.com/kubernetes/kubernetes/blob/v1.5.3/pkg/kubelet/container/container_gc.go#L25){target="_blank"}</br>
</small>

## 

ë™ì‘ë°©ì‹

<span style="color:skyblue;">*image collection*</br></span> 

  * ë§¤ 5ë¶„ ë§ˆë‹¤ ë™ì‘
  * HighThresholdPercent >= disk usage ì‹¤í–‰
  * LowThresholdPercentê¹Œì§€ sizeê°€ ì¤„ë©´ ì¢…ë£Œ

<span style="color:skyblue;">*container collection*</span>

  * ë§¤ 1ë¶„ ë§ˆë‹¤ ë™ì‘
  * minAgeë³´ë‹¤ ì˜¤ë˜ëœ (not running) containerë¥¼ ì œê±°
  * MaxPerPodContainer, MaxContainerê¹Œì§€ ë‚¨ê²¨ë‘ê³  ì •ë¦¬

## 

ì–´ë–»ê²Œ ì ìš©í–ˆë‚˜ - kubelet options
```
apiVersion: kubelet.config.k8s.io/v1beta1
...
imageGCHighThresholdPercent: 85
imageGCLowThresholdPercent: 70
imageMinimumGCAge: 2m0s
maxContainer: -1
maxPerPodContainer: 1  # Crashloopbackoffì‹œ log -p ì˜µì…˜ì„ ìœ„í•œ ì„¤ì •
...
```
<small>
Ref: [kubelet GC](https://kubernetes.io/docs/concepts/cluster-administration/kubelet-garbage-collection/){target="_blank"}
</small>

## 
í•˜ì§€ë§Œ, ê³§ deprecated ëœë‹¤ê³  í•˜ë„¤ìš”...ğŸ˜¢

```
# ê¸°ì¡´ì˜ ì˜µì…˜ì´ ì‚¬ë¼ì§€ê³ , ì´ëŸ°ì‹ìœ¼ë¡œ... 
EvictionHard
  nodefs.available<15%
EvictionSoft
  nodefs.available<25%
  imagefs.available<25%
```
* ë” ìì„¸í•œê±´: [evict options](https://stupefied-goodall-e282f7.netlify.com/contributors/design-proposals/node/kubelet-eviction/){target="_blank"}




#  Graceful eliminate </span>{bg=#123456 .light-on-dark}

## 

<span style="color:skyblue;">*í•„ìš”ì„±*</span>

* ingress(nginx or haproxy)ë¥¼ í†µí•œ ì„œë¹„ìŠ¤ ì¸ì…êµ¬ì¡°
* sh -c "command" container cmdë¡œ server processê°€ SIG TERMì„ ëª»ë°›ìŒ
  * container terminationì€ pid 1ë¡œ SIG TERMë³´ëƒ„
* ì„œë²„ í”„ë¡œì„¸ìŠ¤ëŠ” graceperiod ì´í›„ SIG KILLë¡œ ì£½ìŒ


## 

* pod ì¢…ë£Œì‹œ pre_stop hookì„ ì´ìš©í•´ ë³¼ê¹Œ?

ê·¼ë° ê·¸ëƒ¥ .spec.container.cmdë¥¼ ì„œë²„í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ ëª…ë ¹ì–´ë¡œ ë°”ê¾¸ë©´ ì•ˆë¨?</br>
</br>
ë§ì•„ìš”... ê·¸ê²Œ ì œì¼ ê°„ë‹¨í•©ë‹ˆë‹¤.ğŸ˜‚</br>
ê·¸ë¦¬ì„œ ìš°ë¦¬ë„ ê·¸ê±¸ë¡œ ë°”ê¾¸ë ¤ê³ ìš”.ğŸ‘</br>

# Reservce Resource for system{bg=#123456 .light-on-dark}


##

<span style="color:skyblue;">*í•„ìš”ì„±*</span>

ì²˜ìŒì—” system ì˜ì—­ì˜ ë¦¬ì†ŒìŠ¤ë¥¼ í™•ë³´í•˜ê³ ì í–ˆë‹¤. í•˜ì§€ë§Œ...

##

<span style="color:skyblue;">*ë™ì‘ë°©ì‹*</span>

```render_a2s

Node A Capacity
#-----------------------------#
| kube-reserve |
#-----------------------------#
| system-reserve |
#-----------------------------#
| eviction-threshold |
#-----------------------------#
| allocatable |
| (available for pods) |
#-----------------------------#

```

Nodeì˜ capacityëŠ” ìœ„ ì²˜ëŸ¼ êµ¬ì„±

<small>
* <span style="color:yellow;">kube-reserved</span>: kubelet, container runtime, npd</br>
* <span style="color:yellow;">system-reserved</span>: linux processes, sshd</br>
* <span style="color:yellow;">eviction-threshold</span>: ì•ì—ì„œ ì„¤ëª…í•œ</br>
* <span style="color:yellow;">allocatble</span>: nodeê°€ podì˜ ìŠ¤ì¼€ì¤„ë§ì„ ìœ„í•œ resource</br>
</small>

2ê°œì˜ reservedì™€ thresholdë¥¼ ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ì „ì²´ ë…¸ë“œ ë¦¬ì†ŒìŠ¤ê°€ allocatable

##

ìš°ë¦¬ì˜ ì„¤ì •
```
$ kc describe node [Nodename]
...
Allocatable:
cpu: 40
ephemeral-storage: 857955043546
hugepages-1Gi: 0
hugepages-2Mi: 0
memory: 65582228Ki
pods: 110
...

$ cat /var/lib/kubelet/config.yml
evictionHard:
imagefs.available: 15%
memory.available: 100Mi
nodefs.available: 10%
nodefs.inodesFree: 5%
```
ì™œ? kube-reserved, system-reservedëŠ” í•˜ì§€ ì•Šì£ ?

##
í•˜... ê·¼ë° ì´ê²Œ ë¬¸ì œì¸ê²Œ...

* kube-reserved, system-reservedë¥¼ ì§€ì •í•˜ë©´, í•´ë‹¹ í”„ë¡œì„¸ìŠ¤ë“¤ì€ Bustableí•˜ì§€ ëª»í•¨ë‹ˆë‹¤.
* ì˜¤íˆë ¤ ì„±ëŠ¥ì„ ì €í•˜ì‹œí‚¬ìˆ˜ ìˆì£ , ì˜ë¯¸ì—†ëŠ” evitionì´ ë°œìƒí• ìˆ˜ë„ ìˆê³ 
* ë§Œì•½ ì‚¬ìš©í•˜ê³  ì‹¶ìœ¼ì‹œë‹¤ë©´ ê¸°ì¡´ì˜ ì‚¬ìš©ëŸ‰ì„ ì˜ ëª¨ë‹ˆí„°ë§ í•´ì•¼ í•©ë‹ˆë‹¤.
* ì–´ì°¨í”¼ kube,system-reservedëŠ” priorityë„ ë†’ê³ , oom_score_adjë„ ë‚®ìŠµë‹ˆë‹¤.
* ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ ë˜ëŠ” pod ë³´ë‹¤, evition, oom killë  í™•ë¥ ì´ ë‚®ì£ .

<small>
link: [node v1 api doc](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.13/#nodestatus-v1-core)</br>
link: [pod eviction](https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/#eviction-policy)</br>
link: [reserve resource](https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/)</br>
</small>


